name: Re-upload to Internet Archive

on:
  workflow_dispatch:
    inputs:
      timestamp:
        description: 'The timestamp of the original crawl as a 14-digit YYYYmmddHHMMSS number. Crawl data is usually named "edgi-<timestamp>-<seed_group>".'
        type: string
        required: true
      seed_group:
        description: 'Name of the seed group whose data to upload. Crawl data is usually named "edgi-<timestamp>-<seed_group>".'
        type: string
        required: true

jobs:
  reupload:
    runs-on: ubuntu-latest
    env:
      TIMESTAMP: ${{ inputs.timestamp }}
      COLLECTION: 'edgi-${{ inputs.timestamp }}-${{ inputs.seed_group }}'
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true

      - name: Install Python dependencies
        run: uv sync --all-extras

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v6
        with:
          aws-access-key-id: ${{ secrets.CRAWLER_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.CRAWLER_AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}

      - name: Derive Internet Archive options
        id: ia-options
        run: |
          iso_date="$(
            echo "${TIMESTAMP}" \
            | sed -E 's/^([0-9]{4})([0-9]{2})([0-9]{2}).*/\1-\2-\3/'
          )"
          title="EDGI Web Monitoring Crawl ${iso_date}"

          echo "identifier=edgi-active-urls--${TIMESTAMP}" >> "$GITHUB_OUTPUT"
          echo "iso_date=${iso_date}" >> "$GITHUB_OUTPUT"
          echo "title=${title}" >> "$GITHUB_OUTPUT"

      - name: Download from S3
        run: |
          mkdir -p ./crawls/collections
          aws s3 cp --recursive \
            's3://${{ secrets.CRAWLER_S3_BUCKET }}/${{ env.COLLECTION }}' \
            'crawls/collections/${{ env.COLLECTION }}'

      - name: Upload to Internet Archive
        env:
          IA_ACCESS_KEY_ID: '${{ secrets.IAS3_ACCESS_KEY }}'
          IA_SECRET_ACCESS_KEY: '${{ secrets.IAS3_SECRET_KEY }}'
        run: |
          crawl_path="crawls/collections/${COLLECTION}"
          uv run ia upload \
            --checksum \
            --verify \
            --no-backup \
            --retries 25 \
            --metadata="mediatype:web" \
            --metadata="title:${{ steps.ia-options.outputs.title }}" \
            --metadata="date:${{ steps.ia-options.outputs.iso_date }}" \
            '${{ steps.ia-options.outputs.identifier }}' \
            ${crawl_path}/archive/* \
            ${crawl_path}/logs/*
